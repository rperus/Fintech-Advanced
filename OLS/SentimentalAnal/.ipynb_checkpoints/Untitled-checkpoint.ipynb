{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de84cf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\rperu/nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\.conda\\\\envs\\\\6.16Notebook\\\\nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\.conda\\\\envs\\\\6.16Notebook\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\.conda\\\\envs\\\\6.16Notebook\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bc9203061207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ticker'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mvader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\6.16Notebook\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexicon_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     ):\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVaderConstants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\6.16Notebook\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\6.16Notebook\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\6.16Notebook\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\rperu/nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\.conda\\\\envs\\\\6.16Notebook\\\\nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\.conda\\\\envs\\\\6.16Notebook\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\.conda\\\\envs\\\\6.16Notebook\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\rperu\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AMZN', 'GOOG', 'FB']\n",
    "\n",
    "news_tables = {}\n",
    "for ticker in tickers:\n",
    "    url = finviz_url + ticker\n",
    "\n",
    "    req = Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response = urlopen(req)\n",
    "\n",
    "    html = BeautifulSoup(response, features='html.parser')\n",
    "    news_table = html.find(id='news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "\n",
    "parsed_data = []\n",
    "\n",
    "for ticker, news_table in news_tables.items():\n",
    "\n",
    "    for row in news_table.findAll('tr'):\n",
    "\n",
    "        title = row.a.text\n",
    "        date_data = row.td.text.split(' ')\n",
    "\n",
    "        if len(date_data) == 1:\n",
    "            time = date_data[0]\n",
    "        else:\n",
    "            date = date_data[0]\n",
    "            time = date_data[1]\n",
    "\n",
    "        parsed_data.append([ticker, date, time, title])\n",
    "\n",
    "df = pd.DataFrame(parsed_data, columns=['ticker', 'date', 'time', 'title'])\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "df['compound'] = df['title'].apply(f)\n",
    "df['date'] = pd.to_datetime(df.date).dt.date\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "mean_df = df.groupby(['ticker', 'date']).mean().unstack()\n",
    "mean_df = mean_df.xs('compound', axis=\"columns\")\n",
    "mean_df.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd56bef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\users\\rperu\\.conda\\envs\\6.16notebook\\lib\\site-packages (2021.7.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f707e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8bf358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finviz_url = 'https://finviz.com/crypto_charts.ashx?t='\n",
    "tickers = ['BTCUSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5033d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news_tables = {}\n",
    "for ticker in tickers:\n",
    "    url = finviz_url + ticker\n",
    "\n",
    "    req = Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response = urlopen(req)\n",
    "\n",
    "    html = BeautifulSoup(response, features='html.parser')\n",
    "    news_table = html.find(id='news-table')\n",
    "    news_tables[ticker] = news_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cd94ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-bcc63b74cf2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews_table\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews_tables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "for ticker, news_table in news_tables.items():\n",
    "\n",
    "    for row in news_table.findAll('tr'):\n",
    "\n",
    "        title = row.a.text\n",
    "        date_data = row.td.text.split(' ')\n",
    "\n",
    "        if len(date_data) == 1:\n",
    "            time = date_data[0]\n",
    "        else:\n",
    "            date = date_data[0]\n",
    "            time = date_data[1]\n",
    "\n",
    "        parsed_data.append([ticker, date, time, title])\n",
    "\n",
    "df = pd.DataFrame(parsed_data, columns=['ticker', 'date', 'time', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15053296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Jul-25-21</td>\n",
       "      <td>11:50AM</td>\n",
       "      <td>Dow Jones Futures: Stock Market Rally Back At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Jul-25-21</td>\n",
       "      <td>11:43AM</td>\n",
       "      <td>Big Tech earnings, Federal Reserve decision: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Jul-25-21</td>\n",
       "      <td>11:00AM</td>\n",
       "      <td>Big Tech Earnings Preview: Alphabet, Facebook,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Jul-25-21</td>\n",
       "      <td>09:00AM</td>\n",
       "      <td>3 Vanguard ETFs That Can Help You Survive a Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Jul-25-21</td>\n",
       "      <td>07:30AM</td>\n",
       "      <td>3 Stocks That Can Turn $300,000 into $1 Millio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>FB</td>\n",
       "      <td>Jul-16-21</td>\n",
       "      <td>12:10PM</td>\n",
       "      <td>Why Facebook (FB) is Poised to Beat Earnings E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>FB</td>\n",
       "      <td>Jul-16-21</td>\n",
       "      <td>10:15AM</td>\n",
       "      <td>This Is How Netflix and Facebook Plan to Grab ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>FB</td>\n",
       "      <td>Jul-16-21</td>\n",
       "      <td>09:45AM</td>\n",
       "      <td>10 Best Dividend Stocks to Buy According to Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>FB</td>\n",
       "      <td>Jul-16-21</td>\n",
       "      <td>08:00AM</td>\n",
       "      <td>Facebooks Frustrated Critics Take Their Fight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>FB</td>\n",
       "      <td>Jul-16-21</td>\n",
       "      <td>07:10AM</td>\n",
       "      <td>The Zacks Analyst Blog Highlights: Amazon, Wal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker       date       time  \\\n",
       "0     AMZN  Jul-25-21  11:50AM     \n",
       "1     AMZN  Jul-25-21  11:43AM     \n",
       "2     AMZN  Jul-25-21  11:00AM     \n",
       "3     AMZN  Jul-25-21  09:00AM     \n",
       "4     AMZN  Jul-25-21  07:30AM     \n",
       "..     ...        ...        ...   \n",
       "295     FB  Jul-16-21  12:10PM     \n",
       "296     FB  Jul-16-21  10:15AM     \n",
       "297     FB  Jul-16-21  09:45AM     \n",
       "298     FB  Jul-16-21  08:00AM     \n",
       "299     FB  Jul-16-21  07:10AM     \n",
       "\n",
       "                                                 title  \n",
       "0    Dow Jones Futures: Stock Market Rally Back At ...  \n",
       "1    Big Tech earnings, Federal Reserve decision: W...  \n",
       "2    Big Tech Earnings Preview: Alphabet, Facebook,...  \n",
       "3    3 Vanguard ETFs That Can Help You Survive a Ma...  \n",
       "4    3 Stocks That Can Turn $300,000 into $1 Millio...  \n",
       "..                                                 ...  \n",
       "295  Why Facebook (FB) is Poised to Beat Earnings E...  \n",
       "296  This Is How Netflix and Facebook Plan to Grab ...  \n",
       "297  10 Best Dividend Stocks to Buy According to Fr...  \n",
       "298  Facebooks Frustrated Critics Take Their Fight ...  \n",
       "299  The Zacks Analyst Blog Highlights: Amazon, Wal...  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6d2220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://finviz.com/crypto_charts.ashx?t=BTCUSD'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c710748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
